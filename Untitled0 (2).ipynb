{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Support Vector Machine (SVM), and how does it work?\n",
        "\n",
        "Ans A support vector machine (SVM) is a supervised machine learning algorithm that classifies data by finding an optimal line or hyperplane that maximizes the distance between each class in an N-dimensional space.SVMs are commonly used within classification problems. They distinguish between two classes by finding the optimal hyperplane that maximizes the margin between the closest data points of opposite classes. The number of features in the input data determine if the hyperplane is a line in a 2-D space or a plane in a n-dimensional space. Since multiple hyperplanes can be found to differentiate classes, maximizing the margin between points enables the algorithm to find the best decision boundary between classes."
      ],
      "metadata": {
        "id": "9CQDyqUrQQYp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "\n",
        "Ans **Hard Margin SVM**\n",
        "\n",
        "**Goal:** To find a hyperplane that perfectly separates the data points of different classes with no misclassifications.\n",
        "\n",
        "**Assumption:** Assumes the data is linearly separable without any errors or outliers.\n",
        "\n",
        "**Characteristics:**\n",
        "\n",
        "Maximizes the margin between the classes.\n",
        "\n",
        "Highly sensitive to outliers, as even a single misclassified point can significantly alter the decision boundary.\n",
        "\n",
        "Fails to find a hyperplane if the data is not perfectly linearly separable.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Soft Margin SVM**\n",
        "\n",
        "**Goal:** To find a hyperplane that allows for some margin violations (misclassifications) to achieve better performance on noisy or non-linearly separable data.\n",
        "\n",
        "**Assumption:** Handles cases where the data is not perfectly linearly separable or contains outliers.\n",
        "\n",
        "**Characteristics:**\n",
        "\n",
        "Introduces slack variables, which measure the degree of violation of the margin for each data point.\n",
        "\n",
        "Uses a regularization parameter (often denoted as C) to control the trade-off between maximizing the margin and minimizing the misclassification errors.\n",
        "\n",
        "More flexible and robust to outliers.\n",
        "\n",
        "Generally performs better on real-world data by avoiding overfitting to noisy points."
      ],
      "metadata": {
        "id": "CJrzuDSRQQgE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and\n",
        "explain its use case.\n",
        "\n",
        "Ans The Kernel Trick in Support Vector Machines (SVMs) allows them to classify non-linear data by implicitly mapping it into a higher-dimensional space where it becomes linearly separable. By calculating the dot product (kernel function) in this high-dimensional space without explicit transformation, the SVM can find a linear decision boundary, effectively creating a non-linear boundary in the original space.\n",
        "\n",
        "\n",
        "---\n",
        " **How the Kernel Trick Works**\n",
        "\n",
        "**Problem with Non-Linear Data:** Standard SVMs use linear classifiers, which struggle with datasets that are not linearly separable (i.e., cannot be separated by a straight line).\n",
        "\n",
        "**Implicit Transformation:** The kernel trick avoids explicitly transforming data to a higher-dimensional space. Instead, it uses a kernel function to compute the dot product between data points as if they were in a higher-dimensional space.\n",
        "\n",
        "**Higher-Dimensional Separation:** This implicit mapping into a higher-dimensional space allows for the creation of a linear decision boundary that, when projected back into the original lower-dimensional space, appears as a non-linear boundary.\n",
        "\n",
        "---\n",
        "**Example:**\n",
        "The Radial Basis Function (RBF) Kernel:\n",
        "\n",
        "**Kernel Function:** The RBF kernel is a popular choice.\n",
        "\n",
        "**Use Case:**\n",
        " Consider a 2D dataset where data points of different classes are intertwined in a way that cannot be separated by a single straight line.\n",
        "The RBF kernel can project this 2D data into a 3D (or even higher) dimensional space, making the data points linearly separable.\n",
        "In this higher dimension, similar points are grouped together, allowing a linear plane to divide them.\n",
        "This linear plane in the higher dimension corresponds to a non-linear decision boundary in the original 2D space, allowing the SVM to classify the complex, non-linear data effectively.\n",
        "\n"
      ],
      "metadata": {
        "id": "sIc8zG1YQQm5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What is a Naïve Bayes Classifier, and why is it called “naïve”?\n",
        "\n",
        "Ans It is a supervised machine learning algorithm, meaning it learns from labeled training data to make classifications.\n",
        "\n",
        "It's a probabilistic classifier, meaning it predicts the probability of a given data point belonging to a particular class.\n",
        "\n",
        "The \"Bayes\" part of the name refers to its reliance on Bayes' Theorem to calculate these probabilities.\n",
        "\n",
        "---\n",
        "**Why is it called \"Naïve\"?**\n",
        "\n",
        "The term \"naïve\" comes from the core assumption that the features are independent of each other, given the class label.\n",
        "\n",
        "**For example**, when classifying a fruit as an apple based on its color (red) and shape (round), the classifier assumes that the red color doesn't affect the shape, and neither affects the other in terms of predicting it's an apple.\n",
        "\n",
        "This assumption of independence is often not true in real-world data, where features can be highly correlated. However, despite this simplification, Naïve Bayes often performs surprisingly well in practice, especially with large datasets and for text classification"
      ],
      "metadata": {
        "id": "C0MCCdu_QQqh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants.\n",
        "When would you use each one?\n",
        "\n",
        "Ans **1. Gaussian Naïve Bayes**\n",
        "\n",
        "**Description:**\n",
        "\n",
        " Assumes features follow a Gaussian (normal) distribution.\n",
        "**How it works:**\n",
        "\n",
        " The model learns the mean and standard deviation of each feature for each class to classify new data.\n",
        "\n",
        "**When to use:**\n",
        "\n",
        " Use when your features are continuous and are normally distributed, such as physical measurements like height or weight.\n",
        "\n",
        " ---\n",
        "**2. Multinomial Naïve Bayes**\n",
        "\n",
        "**Description:**\n",
        "\n",
        " Deals with discrete data and uses feature frequencies to make predictions.\n",
        "\n",
        "**How it works:**\n",
        "\n",
        " It models features as counts, making it suitable for situations like document classification where the frequency of words is important.\n",
        "\n",
        "**When to use:**\n",
        "\n",
        " Ideal for text classification and other problems with discrete features representing counts, such as word counts in a document or occurrences of a category.\n",
        "\n",
        "---\n",
        "**3. Bernoulli Naïve Bayes**\n",
        "\n",
        "**Description:**\n",
        "\n",
        "Assumes features are binary (0 or 1), representing the presence or absence of an item.\n",
        "\n",
        "**How it works:**\n",
        "\n",
        " It considers only the binary outcome of a feature, rather than its frequency.\n",
        "\n",
        "**When to use:**\n",
        "\n",
        " Best for binary data or when you only need to know if a feature is present or absent, such as determining if a word appears in a document (regardless of how many times)."
      ],
      "metadata": {
        "id": "-EWF2-XeQQxW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to:\n",
        "\n",
        "● Load the Iris dataset\n",
        "\n",
        "● Train an SVM Classifier with a linear kernel\n",
        "\n",
        "● Print the model's accuracy and support vectors.\n"
      ],
      "metadata": {
        "id": "6F1V3_1ZQQ06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train an SVM Classifier with a linear kernel\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "y_pred = svm_linear.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the model's accuracy and support vectors\n",
        "print(f\"Accuracy of the SVM with linear kernel: {accuracy:.2f}\")\n",
        "print(\"Support Vectors:\")\n",
        "print(svm_linear.support_vectors_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OvJOtf4Umd1",
        "outputId": "ff56c532-0408-45ba-f485-d824f555d3a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the SVM with linear kernel: 1.00\n",
            "Support Vectors:\n",
            "[[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "\n",
        "● Load the Breast Cancer dataset\n",
        "\n",
        "● Train a Gaussian Naïve Bayes model\n",
        "\n",
        "● Print its classification report including precision, recall, and F1-score.\n"
      ],
      "metadata": {
        "id": "ZMYK9vHHVKOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Gaussian Naïve Bayes model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Print the classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0l7SgnFVRDH",
        "outputId": "5742431e-ec8b-4df5-a332-007dd74b6edb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.90      0.92        63\n",
            "           1       0.95      0.96      0.95       108\n",
            "\n",
            "    accuracy                           0.94       171\n",
            "   macro avg       0.94      0.93      0.94       171\n",
            "weighted avg       0.94      0.94      0.94       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "\n",
        "● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "C and gamma.\n",
        "\n",
        "● Print the best hyperparameters and accuracy.\n"
      ],
      "metadata": {
        "id": "kUHzU3drVfvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {'C': [0.1, 1, 10, 100],\n",
        "              'gamma': [1, 0.1, 0.01, 0.001],\n",
        "              'kernel': ['rbf']}\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=2)\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(grid.best_params_)\n",
        "\n",
        "# Make predictions with the best estimator and calculate accuracy\n",
        "grid_predictions = grid.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, grid_predictions)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Accuracy with Best Hyperparameters: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIIvbhtEVl7S",
        "outputId": "f0e94bfa-f176-43c6-a705-f640d76d54ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
            "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "Best Hyperparameters:\n",
            "{'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Accuracy with Best Hyperparameters: 0.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "\n",
        "● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "\n",
        "● Print the model's ROC-AUC score for its predictions."
      ],
      "metadata": {
        "id": "ZEendc16VuHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "# Load a subset of the 20 Newsgroups dataset\n",
        "# We select two categories to make it a binary classification problem for ROC-AUC\n",
        "categories = ['alt.atheism', 'soc.religion.christian']\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "X_train = newsgroups_train.data\n",
        "y_train = newsgroups_train.target\n",
        "X_test = newsgroups_test.data\n",
        "y_test = newsgroups_test.target\n",
        "\n",
        "# Convert text data to feature vectors using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vectors = vectorizer.fit_transform(X_train)\n",
        "X_test_vectors = vectorizer.transform(X_test)\n",
        "\n",
        "# Train a Multinomial Naïve Bayes model\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(X_train_vectors, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class (for ROC-AUC)\n",
        "y_pred_proba = mnb.predict_proba(X_test_vectors)[:, 1]\n",
        "\n",
        "# Binarize the true labels for ROC-AUC calculation\n",
        "lb = LabelBinarizer()\n",
        "y_test_binarized = lb.fit_transform(y_test)\n",
        "\n",
        "# Calculate the ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test_binarized, y_pred_proba)\n",
        "\n",
        "# Print the ROC-AUC score\n",
        "print(f\"ROC-AUC Score: {roc_auc:.2f}\")"
      ],
      "metadata": {
        "id": "q4PdgTb0VztQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a company that handles\n",
        "email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "contain:\n",
        "\n",
        "● Text with diverse vocabulary\n",
        "\n",
        "● Potential class imbalance (far more legitimate emails than spam)\n",
        "\n",
        "● Some incomplete or missing data\n",
        "\n",
        "Explain the approach you would take to:\n",
        "\n",
        "● Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "\n",
        "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "\n",
        "● Address class imbalance\n",
        "\n",
        "● Evaluate the performance of your solution with suitable metrics\n",
        "\n",
        "And explain the business impact of your solution."
      ],
      "metadata": {
        "id": "en0piZodpEYr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7wPiSpipwe_"
      },
      "source": [
        "Here's an approach to building an email spam classification system:\n",
        "\n",
        "**1. Preprocessing the Data**\n",
        "\n",
        "*   **Text Vectorization:** Since emails contain text, we need to convert it into numerical features that machine learning models can understand. Techniques like **TF-IDF (Term Frequency-Inverse Document Frequency)** are suitable for handling diverse vocabulary. TF-IDF assigns weights to words based on their frequency in a document and across the entire dataset, capturing the importance of words.\n",
        "*   **Handling Missing Data:** Incomplete or missing data in emails (e.g., missing subject lines, empty body) need to be addressed. Depending on the nature and extent of missing data, strategies include:\n",
        "    *   **Imputation:** Replacing missing values with a placeholder or a statistically derived value (e.g., the most frequent value for categorical data, mean/median for numerical data - though less common for text).\n",
        "    *   **Removal:** If the amount of missing data is small and doesn't significantly impact the dataset, rows or columns with missing values can be removed.\n",
        "    *   **Treating missingness as a feature:** Creating a binary feature indicating whether a value was missing.\n",
        "\n",
        "**2. Choosing and Justifying an Appropriate Model (SVM vs. Naïve Bayes)**\n",
        "\n",
        "Both SVM and Naïve Bayes are viable options, but for email classification with potentially diverse vocabulary, **Multinomial Naïve Bayes** is often a good starting point and a strong candidate.\n",
        "\n",
        "*   **Naïve Bayes (Multinomial):**\n",
        "    *   **Pros:** Simple, computationally efficient, works well with high-dimensional data (like text), and performs surprisingly well in many text classification tasks. The \"naïve\" assumption of feature independence often doesn't hurt performance significantly in practice for text. Multinomial Naïve Bayes is specifically designed for discrete features like word counts or TF-IDF values.\n",
        "    *   **Cons:** The independence assumption can be a limitation if there are strong dependencies between words that are crucial for classification.\n",
        "*   **SVM:**\n",
        "    *   **Pros:** Effective in high-dimensional spaces, can use various kernels to handle non-linear relationships, and often provides good performance.\n",
        "    *   **Cons:** Can be computationally more expensive to train than Naïve Bayes, especially on large datasets. Choosing the right kernel and hyperparameters can be challenging.\n",
        "\n",
        "Given the potential for a large dataset (many emails) and the nature of text data (high dimensionality), **Multinomial Naïve Bayes** is a good initial choice due to its efficiency and proven performance in text classification. However, SVM with an appropriate kernel (like the linear kernel for high dimensions or RBF for potential non-linearity) could also be explored and compared.\n",
        "\n",
        "**Justification:** Multinomial Naïve Bayes aligns well with the characteristics of text data represented by TF-IDF, and its computational efficiency is a significant advantage for handling large volumes of emails.\n",
        "\n",
        "**3. Addressing Class Imbalance**\n",
        "\n",
        "Email datasets often have a significant class imbalance (many more legitimate emails than spam). This can lead to models that are biased towards the majority class (not spam) and perform poorly on the minority class (spam). Strategies to address this include:\n",
        "\n",
        "*   **Resampling Techniques:**\n",
        "    *   **Oversampling the minority class:** Duplicating instances of spam emails or generating synthetic spam instances (e.g., using SMOTE - Synthetic Minority Over-sampling Technique).\n",
        "    *   **Undersampling the majority class:** Randomly removing instances of legitimate emails.\n",
        "*   **Using Class Weights:** Many machine learning algorithms allow assigning higher weights to the minority class during training, making misclassifications of spam more costly.\n",
        "*   **Choosing appropriate evaluation metrics:** Focusing on metrics that are less sensitive to class imbalance (see below).\n",
        "\n",
        "**4. Evaluating the Performance of Your Solution with Suitable Metrics**\n",
        "\n",
        "Accuracy is not a sufficient metric for evaluating models on imbalanced datasets. Instead, focus on metrics that provide a clearer picture of performance on both classes:\n",
        "\n",
        "*   **Precision:** Of all emails classified as spam, what percentage were actually spam? (Minimizing false positives is important for not blocking legitimate emails).\n",
        "*   **Recall (Sensitivity):** Of all actual spam emails, what percentage were correctly classified as spam? (Maximizing true positives is important for catching as much spam as possible).\n",
        "*   **F1-Score:** The harmonic mean of precision and recall, providing a balanced measure.\n",
        "*   **ROC-AUC (Receiver Operating Characteristic - Area Under Curve):** Measures the ability of the model to distinguish between the classes. A higher AUC indicates better performance.\n",
        "*   **Confusion Matrix:** A table summarizing the counts of true positives, true negatives, false positives, and false negatives, providing a detailed breakdown of the model's predictions.\n",
        "\n",
        "**5. Business Impact of Your Solution**\n",
        "\n",
        "Implementing an effective spam classification system has significant business impact:\n",
        "\n",
        "*   **Increased Productivity:** Employees spend less time sifting through spam, allowing them to focus on important tasks.\n",
        "*   **Enhanced Security:** Reduces the risk of phishing attacks, malware, and other security threats delivered via spam.\n",
        "*   **Improved User Experience:** Users receive fewer unwanted emails, leading to a cleaner and more efficient inbox.\n",
        "*   **Reduced Costs:** Decreases the need for manual spam filtering and potentially reduces the resources required to deal with the consequences of spam (e.g., data breaches).\n",
        "*   **Better Resource Utilization:** Reduces the storage and bandwidth consumed by spam emails.\n",
        "\n",
        "By effectively classifying emails, the company can improve efficiency, security, and overall user satisfaction, leading to a more productive and secure communication environment."
      ]
    }
  ]
}